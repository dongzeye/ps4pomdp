{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAX_PLATFORMS'] = 'cpu'  # Usually faster than GPU for MCMC in NumPyro (which uses JAX)\n",
    "os.environ['JAX_ENABLE_X64'] = \"True\" # Double precision needed to avoid nan's in posterior sampling\n",
    "\n",
    "import random\n",
    "from experiments.utils import save_configurations\n",
    "\n",
    "from constants import (\n",
    "    MIN_SEED, \n",
    "    MAX_SEED, \n",
    "    SARSOP_POMDPSOL_PATH,\n",
    "    CONFIG_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RiverSwim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIVER_LENGTH = 6\n",
    "HORIZON = 40\n",
    "ENV_PARAMS = {\n",
    "    'transition_params': [0.6, 0.35, 0.05], # P(no move | right), P(went right | right), P(went left | right)\n",
    "    'observation_params': [0.6, 0.2, 0.2], # P(no obs error), P(Left obs error), P(Right obs error)  \n",
    "    'reward_params': [5/1000, 1], # left-most state reward, right-most state reward\n",
    "    'init_state_probs': [1.0, 0, 0, 0, 0, 0],\n",
    "}\n",
    "\n",
    "exp_name = 'river_swim_generic_transitions'\n",
    "\n",
    "n_runs = 20\n",
    "n_episodes = 50 if exp_name == 'test' else 200\n",
    "\n",
    "exp_dir = f'experiments/{exp_name}'\n",
    "result_dir = f'{exp_dir}/results'\n",
    "tmp_dir = f'{exp_dir}/tmp'\n",
    "\n",
    "# Creating configurations\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "config_path = f'{CONFIG_DIR}/{exp_name}.json'\n",
    "\n",
    "random.seed(exp_name)\n",
    "\n",
    "planner_discount = 0.95 if 'discounted' in exp_name else 0.9999\n",
    "env_discount = planner_discount if 'discounted' in exp_name else 1.0\n",
    "\n",
    "global_config = {\n",
    "    'exp_name': exp_name,\n",
    "    'tmp_dir': tmp_dir,\n",
    "    'result_dir': result_dir,\n",
    "    'n_runs': n_runs,\n",
    "    'n_episodes': n_episodes,\n",
    "    'env_config': {\n",
    "        'id': \"pomdp_envs:RiverSwim-v0\",\n",
    "        'discount': env_discount,\n",
    "        'horizon': HORIZON,\n",
    "        'river_length': RIVER_LENGTH,\n",
    "        'params': ENV_PARAMS,\n",
    "    },\n",
    "    'infer_config': {\n",
    "        'prior_name': 'river_swim_pomdp_prior_generic_transitions',\n",
    "        'model_name': 'river_swim_pomdp_model_generic_transitions',\n",
    "        'mcmc_args': {\n",
    "            'num_warmup': 1000,\n",
    "            'num_samples': 2000, \n",
    "            'num_chains': 4,\n",
    "        }\n",
    "    },\n",
    "    'known_params': ['reward_params', 'init_state_probs', 'n_states', 'n_actions', 'n_obs'],\n",
    "    # Eval is to be run after all runs of the online experiments is done, so only one seed is needed.\n",
    "    'eval_seed': random.randint(MIN_SEED, MAX_SEED),\n",
    "    # Eval result stores policy value (i.e., expected return) for each run and each episode.\n",
    "    'eval_result_path': f'{result_dir}/eval_results.csv',\n",
    "}\n",
    "\n",
    "configs = {'global': global_config}\n",
    "\n",
    "for i in range(n_runs):\n",
    "    configs[i] = {\n",
    "        'run_id': i,\n",
    "        'np_seed': random.randint(MIN_SEED, MAX_SEED),\n",
    "        'jr_seed': random.randint(MIN_SEED, MAX_SEED),\n",
    "        'fig_path' : f'{result_dir}/run{i}_eval_plot.jpg',\n",
    "        'log_path':  f'{exp_dir}/run{i}.log',\n",
    "        'result_path' : f'{result_dir}/run{i}_trajectories.pkl',\n",
    "        'planner_config': {\n",
    "            'pomdp_path': f'{tmp_dir}/run{i}.pomdp',\n",
    "            'discount': planner_discount,   # Planner's discount could be different from env\n",
    "            'sarsop_args': {\n",
    "                'pomdpsol_path': SARSOP_POMDPSOL_PATH,\n",
    "                'timeout': 60,\n",
    "                'memory': 1024,\n",
    "                'precision': 0.05,\n",
    "                'policy_path': f'{tmp_dir}/run{i}_sarsop.policy',\n",
    "                'logfile': f'{tmp_dir}/run{i}_sarsop.log',\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "save_configurations(configs, config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random POMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro as npyro\n",
    "from agents.inference import generic_transition_kernel_prior, generic_observation_kernel_prior\n",
    "import jax.random as jr\n",
    "\n",
    "def sample_transition_kernel(key, n_states, n_actions):\n",
    "    return npyro.handlers.seed(generic_transition_kernel_prior, key)(n_states, n_actions)\n",
    "\n",
    "def sample_observation_kernel(key, n_states, n_obs):\n",
    "    return npyro.handlers.seed(generic_observation_kernel_prior, key)(n_states, n_obs)\n",
    "\n",
    "\n",
    "N_STATES = 10\n",
    "N_OBS = N_STATES\n",
    "N_ACTIONS = 4\n",
    "\n",
    "\n",
    "HORIZON = 20\n",
    "KEY = jr.key(2024)\n",
    "repeats = 5\n",
    "\n",
    "for i in range(repeats):\n",
    "    KEY, transit_key, obs_key = jr.split(KEY, 3)\n",
    "    transition_kernel = sample_transition_kernel(transit_key, N_STATES, N_ACTIONS)\n",
    "    observation_kernel = sample_observation_kernel(obs_key, N_OBS, N_STATES)\n",
    "    ENV_PARAMS = {\n",
    "        'transition_kernel': transition_kernel.tolist(), # P(no move | right), P(went right | right), P(went left | right)\n",
    "        'observation_params': [0.6, 0.2, 0.2], # P(no obs error), P(Left obs error), P(Right obs error)  \n",
    "        'reward_params': [5/1000, 1], # left-most state reward, right-most state reward\n",
    "        'init_state_probs': [1.0] + [0] * (N_STATES - 1),\n",
    "    }\n",
    "\n",
    "    exp_name = f'randomSparseRewardPOMDP_{i}'\n",
    "    n_runs = 20\n",
    "    n_episodes = 50 if exp_name == 'test' else 500\n",
    "\n",
    "    exp_dir = f'experiments/{exp_name}'\n",
    "    result_dir = f'{exp_dir}/results'\n",
    "    tmp_dir = f'{exp_dir}/tmp'\n",
    "\n",
    "    # Creating configurations\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "    config_path = f'{CONFIG_DIR}/{exp_name}.json'\n",
    "\n",
    "    random.seed(exp_name)\n",
    "\n",
    "    planner_discount = 0.95 if 'discounted' in exp_name else 0.9999\n",
    "    env_discount = planner_discount if 'discounted' in exp_name else 1.0\n",
    "\n",
    "    global_config = {\n",
    "        'exp_name': exp_name,\n",
    "        'tmp_dir': tmp_dir,\n",
    "        'result_dir': result_dir,\n",
    "        'n_runs': n_runs,\n",
    "        'n_episodes': n_episodes,\n",
    "        'env_config': {\n",
    "            'id': \"pomdp_envs:SparseRewardPOMDP-v0\",\n",
    "            'discount': env_discount,\n",
    "            'horizon': HORIZON,\n",
    "            'n_states': N_STATES,\n",
    "            'n_actions': N_ACTIONS,\n",
    "            'n_obs': N_OBS,\n",
    "            'params': ENV_PARAMS,\n",
    "        },\n",
    "        'infer_config': {\n",
    "            'prior_name': 'generic_pomdp_prior_known_init_state', # 'randomT_pomdp_prior',\n",
    "            'model_name': 'generic_pomdp_model_known_init_state', # 'randomT_pomdp_model',\n",
    "            'mcmc_args': {\n",
    "                'num_warmup': 1000,\n",
    "                'num_samples': 2000, \n",
    "                'num_chains': 4,\n",
    "            }\n",
    "        },\n",
    "        'known_params': ['reward_params', 'init_state_probs', 'n_states', 'n_actions', 'n_obs'],\n",
    "        # Eval is to be run after all runs of the online experiments is done, so only one seed is needed.\n",
    "        'eval_seed': random.randint(MIN_SEED, MAX_SEED),\n",
    "        # Eval result stores policy value (i.e., expected return) for each run and each episode.\n",
    "        'eval_result_path': f'{result_dir}/eval_results.csv',\n",
    "    }\n",
    "\n",
    "    configs = {'global': global_config}\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        configs[i] = {\n",
    "            'run_id': i,\n",
    "            'np_seed': random.randint(MIN_SEED, MAX_SEED),\n",
    "            'jr_seed': random.randint(MIN_SEED, MAX_SEED),\n",
    "            'fig_path' : f'{result_dir}/run{i}_eval_plot.jpg',\n",
    "            'log_path':  f'{exp_dir}/run{i}.log',\n",
    "            'result_path' : f'{result_dir}/run{i}_trajectories.pkl',\n",
    "            'planner_config': {\n",
    "                'pomdp_path': f'{tmp_dir}/run{i}.pomdp',\n",
    "                'discount': planner_discount,   # Planner's discount could be different from env\n",
    "                'sarsop_args': {\n",
    "                    'pomdpsol_path': SARSOP_POMDPSOL_PATH,\n",
    "                    'timeout': 60,\n",
    "                    'memory': 1024,\n",
    "                    'precision': 0.05,\n",
    "                    'policy_path': f'{tmp_dir}/run{i}_sarsop.policy',\n",
    "                    'logfile': f'{tmp_dir}/run{i}_sarsop.log',\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "\n",
    "    save_configurations(configs, config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.2\n",
    "exp_name = f'new_tiger_theta{int(theta * 10)}_discounted'\n",
    "n_runs = 20\n",
    "n_episodes = 20 if exp_name == 'test' else 500\n",
    "horizon = 20\n",
    "\n",
    "exp_dir = './experiments/tiger'\n",
    "config_dir = f'{exp_dir}/configs'\n",
    "result_dir = f'{exp_dir}/{exp_name}/results'\n",
    "tmp_dir = f'{exp_dir}/tmp/{exp_name}'\n",
    "\n",
    "\n",
    "# Creating configurations\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "config_path = f'{config_dir}/{exp_name}.json'\n",
    "\n",
    "random.seed(exp_name)\n",
    "\n",
    "planner_discount = 0.99 if 'discounted' in exp_name else 0.999\n",
    "env_discount = planner_discount if 'discounted' in exp_name else 1.0\n",
    "\n",
    "global_config = {\n",
    "    'exp_name': exp_name,\n",
    "    'exp_dir': exp_dir,\n",
    "    'tmp_dir': tmp_dir,\n",
    "    'result_dir': result_dir,\n",
    "    'n_runs': n_runs,\n",
    "    'n_episodes': n_episodes,\n",
    "    'env_config': {\n",
    "        'id': \"pomdp_envs:Tiger-v0\",\n",
    "        'discount': env_discount,\n",
    "        'horizon': horizon,\n",
    "        'theta': theta,\n",
    "        'listen_cost': -1,\n",
    "        'treasure_reward': 10,\n",
    "        'tiger_penalty': -100,\n",
    "    },\n",
    "    # Planner's discount could be different from env (e.g., when discount = 1, which is not a valid input to SARSOP)\n",
    "    'planner_discount': planner_discount, \n",
    "    'mcmc_config': {\n",
    "        'num_warmup': 5000, \n",
    "        'num_samples': 5000, \n",
    "        'num_chains': 5\n",
    "    }\n",
    "}\n",
    "\n",
    "configs = {'global': global_config}\n",
    "\n",
    "for i in range(n_runs):\n",
    "    configs[i] = {\n",
    "        'run_id': i,\n",
    "        # seed for main experiment to generate policies\n",
    "        'main_seed': random.randint(MIN_SEED, MAX_SEED),\n",
    "        # seed for policy_evaluation \n",
    "        # 'policy_eval_seed': random.randint(MIN_SEED, MAX_SEED),\n",
    "        'fig_path' : f'{result_dir}/run{i}_eval_plot.jpg',\n",
    "        'result_path' : f'{result_dir}/run{i}_results.pkl',\n",
    "        'pomdp_path': f'{result_dir}/run{i}_tiger.pomdp',\n",
    "        'sarsop_config': {\n",
    "            'pomdpsol_path': SARSOP_POMDPSOL_PATH,\n",
    "            'timeout': 60,\n",
    "            'memory': 1024,\n",
    "            'precision': 0.05,\n",
    "            'policy_path': f'{tmp_dir}/run{i}_sarsop.policy',\n",
    "            'logfile': f'{tmp_dir}/run{i}_sarsop.log'\n",
    "        },\n",
    "        'sarsop_optimal_policy_config': {\n",
    "            'pomdpsol_path': SARSOP_POMDPSOL_PATH,\n",
    "            'timeout': 120,\n",
    "            'memory': 2048,\n",
    "            'precision': 0.01,\n",
    "            'policy_path': f'{result_dir}/run{i}_optimal_sarsop.policy',\n",
    "            'logfile': f'{result_dir}/run{i}_optimal_policy_sarsop.log',\n",
    "        },\n",
    "    }\n",
    "\n",
    "save_configurations(configs, config_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps_pomdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
