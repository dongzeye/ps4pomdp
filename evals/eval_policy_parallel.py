import os
# Set necessary environment variables
os.environ['JAX_PLATFORMS'] = 'cpu'  # Usually faster than GPU for MCMC in NumPyro (which uses JAX)
os.environ['JAX_ENABLE_X64'] = "True" # Double precision needed to avoid nan's in posterior sampling

import logging

import random
from itertools import count
import argparse
import numpy as np
import pandas as pd
import ray
import gymnasium as gym

from agents.planners import SARSOPPlanner
from experiments.utils import load_configuration, load_experiment_results

from constants import CONFIG_DIR, MAX_SEED, MIN_SEED

# Define the function to parse command-line arguments
def parse_args():
    parser = argparse.ArgumentParser(
        description="Run Ray for evaluating policies generated by PS4POMDP in the online experiment.")
    parser.add_argument('--exp_name', type=str, help="Experiment name for loading policies")
    parser.add_argument('--n_samples', type=int, default=1, help="Number of samples for evaluating each policy")
    parser.add_argument('--n_runs', type=int, default=None, help="Number of independent runs for the given experiment")
    parser.add_argument('--n_cpus', type=int, default=6, help="Number of CPUs to use for Ray")
    parser.add_argument('--n_gpus', type=int, default=0, help="Number of GPUs to use for Ray")
    args = parser.parse_args()
    return args

# Ray remote function for evlauating POMDP policy
@ray.remote(num_cpus=4)
def eval_policy(seed, env, policy, agent_params, n_samples):
    # Set seed for the worker subprocess
    random.seed(seed)
    np_seed = random.randint(MIN_SEED, MAX_SEED)
    np.random.seed(np_seed)

    # agent_transit_probs and agent_obs_probs are needed for belief updates
    env = env.unwrapped
    if 'transition_kernel' in agent_params:
        agent_transit_probs = agent_params['transition_kernel']
    else:
        agent_transit_probs = env.build_transition_kernel(agent_params['transition_params'])
    
    if 'observation_kernel' in agent_params:
        agent_obs_probs = agent_params['observation_kernel']
    else:
        agent_obs_probs = env.build_observation_kernel(agent_params['observation_params'])
    
    if 'init_state_probs' in agent_params:
        init_belief_state = agent_params['init_state_probs']
    else:
        init_belief_state = env.build_init_state_probs(agent_params['init_state_params'])

    return_history = []
    for _ in range(n_samples):
        obs, _ = env.reset()
        unnorm_belief_state = init_belief_state # initial belief state (will skip normalization later)
        
        total_reward = 0
        for t in count():
            # update belief for the current state after seeing new obs
            unnorm_belief_state = unnorm_belief_state * agent_obs_probs[:, obs]
            
            action = policy(unnorm_belief_state, t)
            obs, reward, terminated, _ ,_ = env.step(action)

            # update belief for the next state after performing action 
            unnorm_belief_state = unnorm_belief_state @ agent_transit_probs[:, action, :]

            # collect reward
            total_reward += reward
            if terminated:
                break
        return_history.append(total_reward)

    return np.mean(return_history)


# Main execution block
if __name__ == "__main__":
    # Parse arguments
    args = parse_args()
    config = load_configuration(f'{CONFIG_DIR}/{args.exp_name}.json')

    # Setup logging
    logging.basicConfig(level=logging.INFO)
    logging.info(f'Evaluating policy values for ' + config['exp_name'])

    # Set seed for reproducibility
    random.seed(config['eval_seed'])
    
    # Compute optimal value
    env = gym.make(**config['env_config']).unwrapped
    env = env.unwrapped
    known_params = {k: getattr(env, k, env.params.get(k)) for k in config['known_params']}
    planner = SARSOPPlanner(**config['planner_config'])
    opt_policy = planner.solve(env)

    exp_results = load_experiment_results(config, args.n_runs)

    # Initialize Ray with specified resources
    ray.init(num_cpus=args.n_cpus, num_gpus=args.n_gpus)
    
    def new_remote(policy, agent_params, n_samples):
        return eval_policy.remote(
            seed=random.randint(MIN_SEED, MAX_SEED),
            env=env.clone(remove_params=False), 
            policy=policy,
            agent_params=agent_params, 
            n_samples=n_samples
        )
    
    # Evaluate the opt_policy first with at least 5000 samples
    futures = [new_remote(opt_policy, env.params, max(args.n_samples, 5000))] 
    keys = []
    for run, trajs in enumerate(exp_results):
        for episode, traj in enumerate(trajs[:310]):
            agent_params = traj['sampled_params'] | known_params
            policy = traj['policy']

            futures.append(new_remote(policy, agent_params, args.n_samples))
            keys.append((run, episode))

    # Run tasks in parallel with ray
    values = ray.get(futures)
  
    # Shut down Ray
    ray.shutdown()

    # Save results into df
    opt_value = values[0]
    policy_values = values[1:]

    df = pd.DataFrame.from_records([
        {'run': run, 'episode': episode, 'expected_return': value}
        for (run, episode), value in zip(keys, policy_values)
    ])

    df['opt_value'] = opt_value
    df['expected_regret'] = df['opt_value'] - df['expected_return']
    df['expected_cum_regret'] = df.groupby('run')['expected_regret'].cumsum()

    df['opt_value_sarsop'] = opt_policy.value().item()
    df['expected_regret_sarsop'] =  df['opt_value_sarsop'] - df['expected_return']
    df['expected_cum_regret_sarsop'] = df.groupby('run')['expected_regret_sarsop'].cumsum()

    save_path = config['eval_result_path']
    df.to_csv(save_path, index=False)
    logging.info(f'Eval results saved to: {save_path}')

    
    
    
