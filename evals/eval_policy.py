import os
# Set necessary environment variables
os.environ['JAX_PLATFORMS'] = 'cpu'  # Usually faster than GPU for MCMC in NumPyro (which uses JAX)
os.environ['JAX_ENABLE_X64'] = "True" # Double precision needed to avoid nan's in posterior sampling

import logging

import random
from itertools import count
import argparse
import numpy as np
import pandas as pd
import gymnasium as gym

from agents.planners import SARSOPPlanner
from experiments.utils import load_configuration, load_experiment_results

from constants import CONFIG_DIR, MAX_SEED, MIN_SEED

def parse_args():
    parser = argparse.ArgumentParser(
        description="Evaluating policies generated by PS4POMDP in the online experiment.")
    parser.add_argument('--exp_name', type=str, help="Experiment name for loading policies")
    parser.add_argument('--n_samples', type=int, default=1, help="Number of samples for evaluating each policy")
    parser.add_argument('--n_runs', type=int, default=None, help="Number of independent runs for the given experiment")
    args = parser.parse_args()
    return args


def eval_policy(seed, env, policy, agent_params, n_samples):
    # Set seed for the worker subprocess
    random.seed(seed)
    np_seed = random.randint(MIN_SEED, MAX_SEED)
    np.random.seed(np_seed)

    # agent_transit_probs and agent_obs_probs are needed for belief updates
    env = env.unwrapped
    if 'transition_kernel' in agent_params:
        agent_transit_probs = agent_params['transition_kernel']
    else:
        agent_transit_probs = env.build_transition_kernel(agent_params['transition_params'])
    
    if 'observation_kernel' in agent_params:
        agent_obs_probs = agent_params['observation_kernel']
    else:
        agent_obs_probs = env.build_observation_kernel(agent_params['observation_params'])
    
    if 'init_state_probs' in agent_params:
        init_belief_state = agent_params['init_state_probs']
    else:
        init_belief_state = env.build_init_state_probs(agent_params['init_state_params'])

    return_history = []
    for _ in range(n_samples):
        obs, _ = env.reset()
        unnorm_belief_state = init_belief_state # initial belief state (will skip normalization later)
        
        total_reward = 0
        for t in count():
            # update belief for the current state after seeing new obs
            unnorm_belief_state = unnorm_belief_state * agent_obs_probs[:, obs]
            
            action = policy(unnorm_belief_state, t)
            obs, reward, terminated, _ ,_ = env.step(action)

            # update belief for the next state after performing action 
            unnorm_belief_state = unnorm_belief_state @ agent_transit_probs[:, action, :]

            # collect reward
            total_reward += reward
            if terminated:
                break
        return_history.append(total_reward)

    return np.mean(return_history)


# Main execution block
if __name__ == "__main__":
    # Parse arguments
    args = parse_args()
    config = load_configuration(f'{CONFIG_DIR}/{args.exp_name}.json')

    # Setup logging
    logging.basicConfig(level=logging.INFO)
    logging.info(f'Evaluating policy values for ' + config['exp_name'])

    # Set seed for reproducibility
    random.seed(config['eval_seed'])
    
    # Compute optimal value
    env = gym.make(**config['env_config']).unwrapped
    env = env.unwrapped
    known_params = {k: getattr(env, k, env.params.get(k)) for k in config['known_params']}
    planner = SARSOPPlanner(**config['planner_config'])
    opt_policy = planner.solve(env)

    exp_results = load_experiment_results(config, args.n_runs)
    
    def eval_policy_conditioned(policy, agent_params, n_samples):
        return eval_policy(
            seed=random.randint(MIN_SEED, MAX_SEED),
            env=env.clone(remove_params=False), 
            policy=policy,
            agent_params=agent_params, 
            n_samples=n_samples
        )
    
    # Evaluate the opt_policy first with at least 5000 samples
    logging.info(f'Working on near-optimal policy')
    opt_value = eval_policy_conditioned(
        opt_policy, env.params, max(args.n_samples, 5000)) 
    policy_values = {}
    for run, trajs in enumerate(exp_results):
        for episode, traj in enumerate(trajs):
            logging.info(f'Working on: run {run}, episode {episode}')

            agent_params = traj['sampled_params'] | known_params
            policy = traj['policy']
            policy_values[run, episode] = eval_policy_conditioned(
                policy, agent_params, args.n_samples)
  

    # Save results into df
    df = pd.DataFrame.from_records([
        {'run': run, 'episode': episode, 'expected_return': value}
        for (run, episode), value in policy_values.items()
    ])

    df['episode'] += 1
    df['opt_value'] = opt_value
    df['regret'] = df['opt_value'] - df['expected_return']
    df['cum_regret'] = df.groupby('run')['regret'].cumsum()
    df['cum_regret/K'] = df['cum_regret'] / df['episode']
    df['cum_regret/sqrtK'] = df['cum_regret'] / np.sqrt(df['episode'])

    save_path = config['eval_result_path']
    df.to_csv(save_path, index=False)
    logging.info(f'Eval results saved to: {save_path}')

    
    
    
